{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGpw2fa0mV2-"
   },
   "outputs": [],
   "source": [
    "import sys, string, pandas as pd, numpy as np, matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7QH0v4fmlEh"
   },
   "source": [
    "# Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tc0BmVCmrJN"
   },
   "source": [
    "## (a) Univariate decision Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OS4XQyQStxXE"
   },
   "source": [
    "## (b) Linear Regression Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Junjo9yuF47"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-AwEpfk2t_lo"
   },
   "source": [
    "## (c) Multimodal Boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWXNAbX0I-tt"
   },
   "source": [
    "# Exercise 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkw8tFAyJEGU"
   },
   "source": [
    "## (a) Logistic Regression with IWLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVlsvJNMPNi2"
   },
   "outputs": [],
   "source": [
    "import sys, string, pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "from scipy.special import expit\n",
    "\n",
    "def sigmoid(value):\n",
    "    return expit(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 108
    },
    "id": "1ShmjkNjdiVw",
    "outputId": "b0376056-a130-45e1-af8c-3ac0c13e5483"
   },
   "outputs": [],
   "source": [
    "def fit_LogReg_IWLS(y, features, eps=0.0000001):\n",
    "    #sample size\n",
    "    N = features.shape[0]\n",
    "    #feature dim\n",
    "    p = features.shape[1] + 1\n",
    "    #initalize beta with 0s\n",
    "    betas = np.zeros(p)\n",
    "\n",
    "    #add column of 1 to features to get our X matrix\n",
    "    X = np.c_[np.ones(N),features]\n",
    "    #compute predictions for class 1: linear combination between input (X) and\n",
    "    # coefficients (betas) passed through the sigmoid function\n",
    "    prob1 = sigmoid( X @ betas)\n",
    "    #compute predictions for class 0 (1.0 - prob1)\n",
    "    prob0 = 1.0 - prob1\n",
    "    #COMPLETE THIS CODE:\n",
    "    W =\n",
    "    old_loglike = 100000\n",
    "    #compute LogLikelihood:\n",
    "    #COMPLETE THIS CODE:\n",
    "    loglike =\n",
    "    niter=0\n",
    "    while np.abs(loglike - old_loglike) > eps:\n",
    "        #update betas:\n",
    "        #COMPLETE THIS CODE\n",
    "        z =\n",
    "        #COMPLETE THIS CODE\n",
    "        betas_new =\n",
    "        betas = betas_new\n",
    "        prob1 = sigmoid( X @ betas)\n",
    "        prob0 = 1.0 - prob1\n",
    "        #make both probs 'stable'\n",
    "        prob1[prob1 == 0.0] = 10**-10\n",
    "        prob0[prob0 == 0.0] = 10**-10\n",
    "\n",
    "        #COMPLETE THIS CODE:\n",
    "        W =\n",
    "        old_loglike = loglike\n",
    "        #COMPLETE THIS CODE:\n",
    "        loglike =\n",
    "        niter += 1\n",
    "    print(\"Total iterations: \" + str(niter))\n",
    "    return(betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhmmLlTsQ2qb"
   },
   "source": [
    "## (b) Logistic Regression with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_TVUdNrQ9EM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "def sigmoid(value):\n",
    "    return expit(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFX1WOjl3rED"
   },
   "outputs": [],
   "source": [
    "#implements gradient descent for a fixed number of iterations\n",
    "#alpha is the learning rate\n",
    "#retunrs the beta coefficiencts\n",
    "#and the loss history\n",
    "def fit_LogReg_GRAD(y, features, alpha=0.001, max_iter=1000):\n",
    "    #sample size\n",
    "    N = features.shape[0]\n",
    "    #feature dim\n",
    "    p = features.shape[1] + 1\n",
    "    #initalize beta with 0s\n",
    "    betas = np.zeros(p)\n",
    "\n",
    "    #add column of 1 to X\n",
    "    X = np.c_[np.ones(N),features]\n",
    "    #compute predictions for class 1\n",
    "    prob1 = sigmoid( X @ betas)\n",
    "    #compute predictions for class 2\n",
    "    prob0 = 1.0 - prob1\n",
    "\n",
    "    #compute the cost (J) of the current solution\n",
    "    #COMPLETE THIS LINE\n",
    "    cost =\n",
    "    loss = []\n",
    "    niter = 0\n",
    "    while niter < max_iter:\n",
    "        #beta update (one step of the gradient descent)\n",
    "        #COMPLETE THIS LINE\n",
    "        betas =\n",
    "        prob1 = sigmoid( X @ betas)\n",
    "        prob0 = 1.0 - prob1\n",
    "        #make both probs 'stable'\n",
    "        prob1[prob1 == 0.0] = 10**-10\n",
    "        prob0[prob0 == 0.0] = 10**-10\n",
    "\n",
    "        #recompute cost\n",
    "        #COMPLETE THIS LINE\n",
    "        cost =\n",
    "        loss.append(cost)\n",
    "        niter += 1\n",
    "    res = {}\n",
    "    res[\"betas\"] = betas\n",
    "    res[\"loss\"] = loss\n",
    "    print(\"total iterations: \" + str(niter))\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MvKg2J7R5Kn"
   },
   "source": [
    "## (c) Logistic Regression with Gradient Descent and Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krW5FLHX4lG0"
   },
   "outputs": [],
   "source": [
    "#implements gradient descent with momentum for a fixed number of iterations\n",
    "#alpha is the learning rate\n",
    "#gamma is the factor for momentum\n",
    "#returns the beta coefficiencts\n",
    "#and the loss history\n",
    "\n",
    "def fit_LogReg_GRAD_momentum(y, features, alpha=0.001, gamma=0.9, max_iter=1000):\n",
    "    #sample size\n",
    "    N = features.shape[0]\n",
    "    #feature dim\n",
    "    p = features.shape[1] + 1\n",
    "    #initalize beta with 0s\n",
    "    betas = np.zeros(p)\n",
    "\n",
    "    #add column of 1 to X\n",
    "    X = np.c_[np.ones(N),features]\n",
    "    #compute predictions for class 1\n",
    "    prob1 = sigmoid( X @ betas)\n",
    "    #compute predictions for class 2\n",
    "    prob0 = 1.0 - prob1\n",
    "\n",
    "    #compute the cost (J) of the current solution\n",
    "    #COMPLETE THIS LINE\n",
    "    cost =\n",
    "    niter = 0\n",
    "    loss = []\n",
    "    while niter < max_iter:\n",
    "        #beta update (one step of the gradient descent)\n",
    "        #COMPLETE THIS LINE\n",
    "        betas =\n",
    "        prob1 = sigmoid( X @ betas)\n",
    "        prob0 = 1.0 - prob1\n",
    "        #make both probs 'stable'\n",
    "        prob1[prob1 == 0.0] = 10**-10\n",
    "        prob0[prob0 == 0.0] = 10**-10\n",
    "\n",
    "        old_cost = cost\n",
    "        #recompute cost\n",
    "        #COMPLETE THIS LINE\n",
    "        cost =\n",
    "        niter += 1\n",
    "        loss.append(cost)\n",
    "    res = {}\n",
    "    res[\"betas\"] = betas\n",
    "    res[\"loss\"] = loss\n",
    "    print(\"total iterations: \" + str(niter))\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTBORWInSmBm"
   },
   "source": [
    "## (d) Competing Objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmmCdK7y5uB9"
   },
   "outputs": [],
   "source": [
    "#implements gradient descent for a fixed number of iterations\n",
    "#y is the target variable\n",
    "#y2 is the variable to be avoided\n",
    "#delta is the tradoff between the loss for objective1 (i.e., fit y) and objective2 (i.e., don't fit y2)\n",
    "#alpha is the learning rate\n",
    "#returns the beta coefficiencts\n",
    "#and the loss history\n",
    "\n",
    "def fit_LogReg_GRAD_competing(y, y2, features, delta, alpha=0.001, max_iter=1000):\n",
    "    #sample size\n",
    "    N = features.shape[0]\n",
    "    #feature dim\n",
    "    p = features.shape[1] + 1\n",
    "    #initalize beta with 0s\n",
    "    betas = np.zeros(p)\n",
    "\n",
    "    #add column of 1 to X\n",
    "    X = np.c_[np.ones(N),features]\n",
    "    #compute predictions for class 1\n",
    "    prob1 = sigmoid( X @ betas)\n",
    "    #compute predictions for class 2\n",
    "    prob0 = 1.0 - prob1\n",
    "\n",
    "    #compute the cost (J) of the current solution\n",
    "    #COMPLETE THIS LINE\n",
    "    cost =\n",
    "    niter = 0\n",
    "    loss = []\n",
    "    while niter < max_iter:\n",
    "        #beta update (one step of the gradient descent)\n",
    "        #COMPLETE THIS LINE\n",
    "        betas =\n",
    "        prob1 = sigmoid( X @ betas)\n",
    "        prob0 = 1.0 - prob\n",
    "        #make both probs 'stable'\n",
    "        prob1[prob1 == 0.0] = 10**-10\n",
    "        prob0[prob0 == 0.0] = 10**-10\n",
    "        old_cost = cost\n",
    "        #recompute cost\n",
    "        #COMPLETE THIS LINE\n",
    "        cost =\n",
    "        niter += 1\n",
    "        loss.append(cost)\n",
    "    res = {}\n",
    "    res[\"betas\"] = betas\n",
    "    res[\"loss\"] = loss\n",
    "    print(\"total iterations: \" + str(niter))\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qnv47rL-EQU2"
   },
   "source": [
    "# Exercise 3: Prostate segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtHf0Sk9ETcv"
   },
   "outputs": [],
   "source": [
    "#useful to unzip the data when working on Colab\n",
    "import zipfile\n",
    "unzip_data = True\n",
    "\n",
    "if unzip_data:\n",
    "  with zipfile.ZipFile(\"./promise1215.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ISbMTu9_JRdh"
   },
   "outputs": [],
   "source": [
    "import imageio.v3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.filters import gabor, scharr, difference_of_gaussians\n",
    "from skimage.feature import local_binary_pattern\n",
    "from sklearn.feature_extraction import image\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import sklearn\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sw6fc5HiIdhK"
   },
   "outputs": [],
   "source": [
    "data_path = './'\n",
    "train_path= data_path + \"train/\"\n",
    "test_path= data_path + \"test/\"\n",
    "vali_path= data_path + \"validate/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOmIU9aII3Wy"
   },
   "outputs": [],
   "source": [
    "#read image from path\n",
    "def rimg(pt, num, sl=15):\n",
    "    img_f = pt + \"img_\" + str(num).zfill(2) + \"_\" + str(sl).zfill(2) + \".png\"\n",
    "    msk_f = pt + \"lab_\" + str(num).zfill(2) + \"_\" + str(sl).zfill(2) + \".png\"\n",
    "    im  = imageio.v3.imread(img_f)\n",
    "    msk = imageio.v3.imread(msk_f)\n",
    "    return (im, msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2XCX31elJTm6"
   },
   "outputs": [],
   "source": [
    "#read image '04' from the training path\n",
    "img, msk = rimg(train_path, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "-8lRPZT9JWY7",
    "outputId": "146afc08-a593-4338-9cca-41569c273d9d"
   },
   "outputs": [],
   "source": [
    "plt.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "Kh_Kw8pkJZyG",
    "outputId": "45efa430-fe94-48a5-e5fe-69783ae65033"
   },
   "outputs": [],
   "source": [
    "plt.imshow(msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCYi5CSEJfsQ"
   },
   "outputs": [],
   "source": [
    "#this function takes an input image and\n",
    "#does the manual feature extraction\n",
    "#if the variable nchan is sset to True, then the return is just the number of channels\n",
    "\n",
    "def preprocess_img(img, nchan=False):\n",
    "\n",
    "  #raw image and Scharr filter\n",
    "  res = np.dstack( (img, scharr(img) ) )\n",
    "\n",
    "  #add a series fo Gabor filters\n",
    "  for frq in [0.2, 0.4, 0.6, 0.8]:\n",
    "    for j in range(4):\n",
    "      a, b = gabor(img, frq, theta=np.pi/4 * j)\n",
    "      res = np.dstack( (res, np.sqrt(a**2 + b**2) ))\n",
    "  #LBP\n",
    "  radius = 3\n",
    "  n_points = 8 * radius\n",
    "  lbp = local_binary_pattern(img, n_points, radius, method='uniform')\n",
    "\n",
    "  #difference of gaussians\n",
    "  dog = difference_of_gaussians(img, low_sigma=1, high_sigma=3)\n",
    "\n",
    "  res = np.dstack( (res, lbp, dog))\n",
    "\n",
    "  #if nchan is True, then just return the number of channels\n",
    "  #the pre-processing produces\n",
    "  if nchan:\n",
    "    try:\n",
    "      return(res.shape[2])\n",
    "    except IndexError:\n",
    "      return(1)\n",
    "  return (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vI1PHb0PYWjf"
   },
   "outputs": [],
   "source": [
    "#turn the images into a training dataset\n",
    "#train_path:    path to the training data\n",
    "#n_train_patch: number of patches to be extracted per image\n",
    "#ps :           patch size as integer (e.g., 7 -> 7x7 patches)\n",
    "#chan:          number of 'channels' the pre_processed image will have\n",
    "\n",
    "def create_training_set(train_path, n_train_patch, ps, chan, verbose=True):\n",
    "  if verbose:\n",
    "    print(\"Extracting \" + str(n_train_patch) + \" pataches of size \" + str(ps) + \"x\" + str(ps) + \" from each image.\")\n",
    "\n",
    "  #dimension of square patch\n",
    "  patch_dim = (ps, ps)\n",
    "  #center of the patch\n",
    "  patch_cnt = int((ps-1)/2)\n",
    "  #dimension of the 'flattened' patch\n",
    "\n",
    "  flat_dim = ps*ps*chan\n",
    "\n",
    "  #initialize Y and X\n",
    "  Y = []\n",
    "  X = np.empty((0,flat_dim))\n",
    "\n",
    "  for s in np.arange(30):\n",
    "    if verbose:\n",
    "      print(\"Extracting data from subject: \" + str(s))\n",
    "\n",
    "    img_m, msk_x = rimg(train_path, s)\n",
    "\n",
    "    #binarize the mask\n",
    "    msk = (msk_x > 0) * 1\n",
    "\n",
    "    #extract features\n",
    "    frames = preprocess_img(img_m)\n",
    "    #add the labels as an additional channel\n",
    "    frames = np.dstack((frames, msk))\n",
    "\n",
    "    #sample patches\n",
    "    patch = image.extract_patches_2d(frames, patch_dim, max_patches=n_train_patch)\n",
    "\n",
    "    #each patch receives the label of the center value in the 'label' channel\n",
    "    Y_tmp = patch[:,patch_cnt,patch_cnt,chan] == 1\n",
    "    #turn patches into a matrix where each row corresponds to all the features\n",
    "    #of one patch\n",
    "    X_tmp = np.reshape(patch[:,:,:,0:chan],(n_train_patch,flat_dim))\n",
    "\n",
    "    #concatenate with Y and X\n",
    "    Y.extend(Y_tmp)\n",
    "    X = np.concatenate((X, X_tmp), axis=0)\n",
    "\n",
    "    #clear some memory\n",
    "    patch = None\n",
    "    X_tmp = None\n",
    "    Y_tmp = None\n",
    "    gc.collect()\n",
    "  return (Y, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qa27hHs5gcOQ"
   },
   "outputs": [],
   "source": [
    "#this function takes X and Y as input and\n",
    "#oversamples the target class ('1') with a ratio:1 ratio (default: None)\n",
    "# if ratio is None, then it will be set to 1/f, where f is the frequency\n",
    "# of the target class\n",
    "#from the overall size of X a sub_sample of 'sample_size' is randomly selected\n",
    "def sub_sample(X, Y, sample_size=10000, ratio=None):\n",
    "    #compute class 1 frequency\n",
    "    rtclass = np.sum(Y)/len(Y)\n",
    "    if ratio is None:\n",
    "        ratio = 1.0/rtclass\n",
    "        print(\"sampling with ratio 1:\" + str(ratio))\n",
    "    weights = np.array([1] * len(Y))\n",
    "    weights[Y] = ratio\n",
    "    weights = weights/np.sum(weights)\n",
    "\n",
    "    #sample sample_size of the generated patches\n",
    "    idx = np.random.choice(X.shape[0],sample_size,replace=False, p=np.array(weights))\n",
    "\n",
    "    Xsub = X[idx,:]\n",
    "    Ysub = np.array(Y)[idx]\n",
    "\n",
    "    #return subsets of X, Y and the selection index\n",
    "    return Xsub, Ysub, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWbY7usPX1cS"
   },
   "outputs": [],
   "source": [
    "#set number of patches\n",
    "#set patch size\n",
    "n_train_patch = 1000\n",
    "ps = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlKkPgSdYA4v"
   },
   "outputs": [],
   "source": [
    "#get the number of channels\n",
    "nchan = preprocess_img(img, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8Gt82KcZpTZ",
    "outputId": "36ded878-db1f-4a07-df46-b4d3bdfc0a5b"
   },
   "outputs": [],
   "source": [
    "#create the dataset from the 30 training images\n",
    "Y, X = create_training_set(train_path, n_train_patch, ps, nchan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TsqJstlKhStS",
    "outputId": "c4851a96-0064-4e71-f08a-f4f400c1eae3"
   },
   "outputs": [],
   "source": [
    "#subsample as per instructions\n",
    "Xsub, Ysub, sub_idx = sub_sample(X, Y, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "htMjpcI1k2kf",
    "outputId": "a76970f3-ca13-4fcb-8571-e95526b2685b"
   },
   "outputs": [],
   "source": [
    "#the initial X and Y matrices are quite large and\n",
    "#clutter the memory, we don't require these for further processing\n",
    "#so setting them to None and calling the garbage collector\n",
    "#will preserve resources\n",
    "import gc\n",
    "X = None\n",
    "Y = None\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79-uSqtZa2g3"
   },
   "source": [
    "## (a) Train SVC with linear, polynomial and RBF kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKm-Ys_bdQVT"
   },
   "source": [
    "## (b) Apply to validation data, compute DICE and AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8vUfJ0xhDYn"
   },
   "source": [
    "## (c) train a tree-based classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPragRs1sCG0"
   },
   "source": [
    "## (d) Explore sample / patch sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iJuHlEHQy8gK"
   },
   "source": [
    "## (e) Prediction post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjSkwg5MV2e6"
   },
   "source": [
    "## (f) apply to test set"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
